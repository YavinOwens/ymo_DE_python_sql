# Data Engineering with Python and SQL

This comprehensive learning project covers essential skills in data engineering, focusing on Python programming and SQL database management. The project provides hands-on experience with real-world data processing, analysis, and engineering tasks.

## Learning Outcomes

### Technical Skills
- **Python Programming**: Advanced data processing using pandas, numpy, and polars
- **Database Management**: Working with multiple databases (SQLite, PostgreSQL, MySQL, DuckDB)
- **Data Quality**: Implementing tests and validation using great-expectations
- **API Development**: Building REST APIs with FastAPI
- **Documentation**: Creating technical documentation with MkDocs
- **Version Control**: Git-based workflow and best practices
- **Testing**: Unit testing with pytest and code coverage analysis

### Professional Skills
- Data pipeline design and implementation
- ETL process development
- Code organization and best practices
- Performance optimization techniques
- Problem-solving and debugging strategies
- Documentation writing
- Project organization


## Project Structure

```
.
├── data/                   # Data files and databases
├── notebooks/             # Jupyter notebooks for learning
│   ├── Week_1/           # Environment Setup and Basics
│   ├── Week_2/           # Data Loading and Database Operations
│   ├── Week_3/           # Data Analysis and Engineering
│   └── Week_4/           # Integration and Advanced Features
├── scripts/               # Utility and setup scripts
├── src/                   # Source code
├── tests/                 # Test files
├── visualizations/        # Generated visualizations
├── archive/              # Archived files and notebooks
└── docs/                 # Project documentation
```

## Weekly Learning Path

### Week 1: Foundation and Setup
- Development environment setup
- Python basics and virtual environments
- Version control with Git
- Package management and requirements
- Basic SQL operations

### Week 2: Data Processing and Storage
- Database connections (SQLite, PostgreSQL, MySQL)
- SQLAlchemy ORM
- Data loading and transformation
- Error handling and validation
- Query optimization

### Week 3: Advanced Data Engineering
- ETL pipeline development
- Data quality checks with great-expectations
- Performance optimization
- Parallel processing
- API development with FastAPI

### Week 4: Integration and Best Practices
- Testing and code coverage
- Documentation with MkDocs
- Logging and monitoring
- Deployment strategies
- Code quality and linting
- Project integration

## Setup and Installation

1. Clone the repository
2. Create a virtual environment:
   ```bash
   python -m venv .venv
   .venv\Scripts\activate  # Windows
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

## Tools and Technologies

### Core Technologies
- Python 3.8+
- SQL (SQLite, PostgreSQL, MySQL)
- Git

### Key Libraries
- **Data Processing**: pandas, numpy, polars
- **Databases**: SQLAlchemy, pyodbc, psycopg2
- **API Development**: FastAPI, uvicorn
- **Testing**: pytest, pytest-cov
- **Documentation**: MkDocs, mkdocs-material
- **Code Quality**: black, flake8, isort

## Best Practices Implemented

- Modular code organization
- Comprehensive documentation
- Test-driven development
- Code quality checks
- Version control workflow
- Project structure standards
- Error handling and logging
- Performance optimization

## Getting Started

1. Review the project structure and requirements
2. Set up your development environment using the setup scripts
3. Follow the notebooks in sequential order
4. Complete the exercises in each notebook
5. Build your own data engineering projects using these patterns

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

