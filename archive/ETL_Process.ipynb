{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Process for HR Data\n",
    "\n",
    "This notebook implements an ETL (Extract, Transform, Load) process for HR data, following best practices and PEP standards.\n",
    "\n",
    "## Process Overview:\n",
    "1. Extract: Read data from CSV files\n",
    "2. Transform: Clean, validate, and structure data\n",
    "3. Load: Create relational database structure and load processed data\n",
    "\n",
    "### Features:\n",
    "- Proper error handling and logging\n",
    "- Type hints (PEP 484)\n",
    "- Docstrings (PEP 257)\n",
    "- Class-based design\n",
    "- Data validation and cleaning\n",
    "- Relational database with proper foreign keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 12:18:01,772 - INFO - Successfully loaded HR_ALL_ADDRESSES_20241216.csv\n",
      "2024-12-16 12:18:01,772 - INFO - Added DataFrame for HR\n",
      "2024-12-16 12:18:01,776 - INFO - Successfully loaded PER_ALL_ASSIGNMENTS_F_20241216.csv\n",
      "2024-12-16 12:18:01,777 - INFO - Added DataFrame for PER\n",
      "2024-12-16 12:18:01,783 - INFO - Successfully loaded PER_ALL_PEOPLE_F_20241216.csv\n",
      "2024-12-16 12:18:01,784 - INFO - Added DataFrame for PER\n",
      "2024-12-16 12:18:01,785 - INFO - Added index to HR\n",
      "2024-12-16 12:18:01,786 - INFO - Added index to PER\n",
      "2024-12-16 12:18:01,787 - INFO - Cleaned column names for HR\n",
      "2024-12-16 12:18:01,788 - INFO - Cleaned column names for PER\n",
      "2024-12-16 12:18:01,796 - INFO - Processed people data\n",
      "2024-12-16 12:18:01,798 - INFO - Processed assignments data\n",
      "2024-12-16 12:18:01,802 - INFO - Processed addresses data\n",
      "2024-12-16 12:18:01,803 - INFO - Connected to database\n",
      "2024-12-16 12:18:01,822 - INFO - Created table: per\n",
      "2024-12-16 12:18:01,838 - INFO - Created table: assignments\n",
      "2024-12-16 12:18:01,852 - INFO - Created table: addresses\n",
      "2024-12-16 12:18:01,853 - INFO - Created table relationships\n",
      "2024-12-16 12:18:01,854 - INFO - Closed database connection\n",
      "2024-12-16 12:18:01,854 - INFO - ETL process completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the ETL process code\n",
    "with open('etl_process.py', 'r') as file:\n",
    "    etl_code = file.read()\n",
    "\n",
    "# Execute the ETL code\n",
    "exec(etl_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the ETL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 12:18:01,873 - INFO - Successfully loaded HR_ALL_ADDRESSES_20241216.csv\n",
      "2024-12-16 12:18:01,874 - INFO - Added DataFrame for HR\n",
      "2024-12-16 12:18:01,877 - INFO - Successfully loaded PER_ALL_ASSIGNMENTS_F_20241216.csv\n",
      "2024-12-16 12:18:01,879 - INFO - Added DataFrame for PER\n",
      "2024-12-16 12:18:01,883 - INFO - Successfully loaded PER_ALL_PEOPLE_F_20241216.csv\n",
      "2024-12-16 12:18:01,884 - INFO - Added DataFrame for PER\n",
      "2024-12-16 12:18:01,884 - INFO - Added index to HR\n",
      "2024-12-16 12:18:01,885 - INFO - Added index to PER\n",
      "2024-12-16 12:18:01,886 - INFO - Cleaned column names for HR\n",
      "2024-12-16 12:18:01,886 - INFO - Cleaned column names for PER\n",
      "2024-12-16 12:18:01,891 - INFO - Processed people data\n",
      "2024-12-16 12:18:01,895 - INFO - Processed assignments data\n",
      "2024-12-16 12:18:01,901 - INFO - Processed addresses data\n",
      "2024-12-16 12:18:01,903 - INFO - Connected to database\n",
      "2024-12-16 12:18:01,918 - INFO - Created table: per\n",
      "2024-12-16 12:18:01,932 - INFO - Created table: assignments\n",
      "2024-12-16 12:18:01,946 - INFO - Created table: addresses\n",
      "2024-12-16 12:18:01,947 - INFO - Created table relationships\n",
      "2024-12-16 12:18:01,948 - INFO - Closed database connection\n",
      "2024-12-16 12:18:01,949 - INFO - ETL process completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Run the main ETL process\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data Loading\n",
    "\n",
    "Let's verify that our data was loaded correctly by running some example queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record counts:\n",
      "per: 1000 records\n",
      "assignments: 1000 records\n",
      "addresses: 1000 records\n",
      "\n",
      "Sample people data:\n",
      "              name                         job                   company  age\n",
      "0     Sharon Jones                      Dancer                Walker PLC   13\n",
      "1      Amy Roberts                Video editor  Herman, Henry and Taylor    5\n",
      "2  Andrew Meyer MD                      Writer              Callahan Inc   33\n",
      "3   Shannon Hansen           Social researcher               Roberts Inc   35\n",
      "4     Julie Taylor  Civil engineer, consulting         Williamson-Bryant   12\n",
      "\n",
      "Sample addresses:\n",
      "                 street_address         city                           country\n",
      "0           7738 Oconnor Forges    Tatehaven                           Iceland\n",
      "1   05547 Hodges Port Suite 337  Anthonyberg                              Oman\n",
      "2  7896 Sabrina Union Suite 137   Garzahaven                  Marshall Islands\n",
      "3              453 Michael Rest   Sandrabury                            Bhutan\n",
      "4               3627 Lewis Oval  Simmonsfurt  Saint Vincent And The Grenadines\n"
     ]
    }
   ],
   "source": [
    "def verify_data():\n",
    "    \"\"\"Run verification queries on the loaded data.\"\"\"\n",
    "    conn = sqlite3.connect('hr_database.sqlite')\n",
    "    \n",
    "    # Query 1: Count records in each table\n",
    "    tables = ['per', 'assignments', 'addresses']\n",
    "    print(\"Record counts:\")\n",
    "    for table in tables:\n",
    "        query = f\"SELECT COUNT(*) as count FROM {table}\"\n",
    "        count = pd.read_sql_query(query, conn).iloc[0, 0]\n",
    "        print(f\"{table}: {count} records\")\n",
    "    \n",
    "    # Query 2: Sample data from people table\n",
    "    print(\"\\nSample people data:\")\n",
    "    query = \"SELECT name, job, company, age FROM per LIMIT 5\"\n",
    "    print(pd.read_sql_query(query, conn))\n",
    "    \n",
    "    # Query 3: Sample addresses\n",
    "    print(\"\\nSample addresses:\")\n",
    "    query = \"SELECT street_address, city, country FROM addresses LIMIT 5\"\n",
    "    print(pd.read_sql_query(query, conn))\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "verify_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Schema\n",
    "\n",
    "The ETL process creates the following tables:\n",
    "\n",
    "1. `per` - People information\n",
    "   - per_id (PRIMARY KEY)\n",
    "   - name\n",
    "   - nino (National Insurance Number)\n",
    "   - job\n",
    "   - company\n",
    "   - date_of_birth\n",
    "   - age\n",
    "\n",
    "2. `assignments` - Job assignments\n",
    "   - assignments_id (PRIMARY KEY)\n",
    "   - job\n",
    "   - company\n",
    "   - date_joined\n",
    "\n",
    "3. `addresses` - Address information\n",
    "   - addresses_id (PRIMARY KEY)\n",
    "   - street_address\n",
    "   - city\n",
    "   - country\n",
    "   - postcode\n",
    "\n",
    "4. `person_assignments` - Relationship table\n",
    "   - id (PRIMARY KEY)\n",
    "   - per_id (FOREIGN KEY)\n",
    "   - assignment_id (FOREIGN KEY)\n",
    "\n",
    "5. `person_addresses` - Relationship table\n",
    "   - id (PRIMARY KEY)\n",
    "   - per_id (FOREIGN KEY)\n",
    "   - address_id (FOREIGN KEY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
